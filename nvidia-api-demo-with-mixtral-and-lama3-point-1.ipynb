{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {},
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Elastic Supertransformation Platform Demo: Calling nvapi from Elastic Provisioner Transformer\n\n",
        "This notebook demonstrates how to use `nvapi` with an elastic provisioner to generate output from a model. We will provide examples using different models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n\n",
        "Ensure that you have `nvapi` and `eprov` installed and properly configured in your environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import subprocess\n\n",
        "# Define the function to install Elastic Provisioner\n",
        "def install_eprov():\n",
        "    command = 'curl https://api.elasticprovisioner.com/install-eprov | bash'\n\n",
        "    # Execute the command\n",
        "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
        "    # Return the result\n",
        "    return result.stdout\n\n",
        "# Run the function and display the result\n",
        "output = install_eprov()\n",
        "print(\"Command Output:\\n\", output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic EPT Usage Example\n\n",
        "We'll start with a basic example that echoes a rhyme about GPUs using the specified model. Here we're using Elastic Provisioner Transformer on the Elastic Supertransformation Platform to call these models via Nvidia API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import subprocess\n\n",
        "# Define the function to run the command\n",
        "def run_nvapi_command():\n",
        "    # Define the command to use the default model.\n",
        "    command = 'echo \"make a rhyme about GPUs\" | eprov ept nvapi'\n\n",
        "    # Execute the command\n",
        "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n\n",
        "    # Return the result\n",
        "    return result.stdout\n\n",
        "# Run the function and display the result\n",
        "output = run_nvapi_command()\n",
        "print(\"Command Output:\\n\", output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example with Multiple Models\n\n",
        "You can specify multiple models in a single `nvapi::` call by enclosing them in single quotes after the `::`. In this example, we use both the `mixtral` model and the default NVIDIA model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def run_nvapi_with_multiple_models():\n",
        "    # call two models and get both results\n",
        "    command = (\n",
        "        'echo \"make a rhyme about GPUs\" | eprov ept nvapi::model=mistralai/mixtral-8x22b-instruct-v0.1 nvapi::model=meta/llama-3.1-8b-instruct'\n",
        "    )\n\n",
        "    # Execute the command\n",
        "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n\n",
        "    # Return the result\n",
        "    return result.stdout\n\n",
        "# Run the function and display the result\n",
        "output = run_nvapi_with_multiple_models()\n",
        "print(\"Command Output:\\n\", output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\n",
        "This notebook provides a basic guide to using `nvapi` with an elastic provisioner, along with examples of specifying different models. Customize the commands and parameters as needed for your specific use case.\n\n",
        "This notebook focuses on:\n",
        "- Specifying different models, including both `mixtral` and the default NVIDIA model, in a single call.\n",
        "- A section to read and display the content of the uploaded file for additional information."
      ]
    }
  ]
}
